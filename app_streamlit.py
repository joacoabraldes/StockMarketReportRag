# app_streamlit.py
# -*- coding: utf-8 -*-
from dotenv import load_dotenv
load_dotenv()
import os, json, datetime
from typing import List, Tuple, Dict, Any, Optional
import streamlit as st
import pandas as pd

# langchain HuggingFace embeddings
from langchain_huggingface import HuggingFaceEmbeddings

# openai optional
try:
    from openai import OpenAI
except Exception:
    OpenAI = None

from core.utils import clean_text, fetch_url_text, SimpleDoc, extract_date_from_text, format_variations_for_prompt, df_to_single_doc
from core.evaluator import load_dataset, build_eval_prompt, call_evaluator, extract_date_from_prompt, find_reference_for_date
from core.compute_variations import compute_variations
from config.market_config import get_market_config
from core.debug_logger import DebugSession

# --------- Config / defaults ----------
st.set_page_config(page_title="PARROT RAG", layout="wide")
st.title("ü¶ú PARROT RAG ‚Äî informe de rueda (CSV -> resumen)")

DEFAULT_EMBED     = os.environ.get("EMBEDDING_MODEL", "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
DEFAULT_OAI_MODEL = os.environ.get("OPENAI_MODEL", "gpt-4o-mini")

# --------- Sidebar ----------
st.sidebar.header("‚öôÔ∏è Configuraci√≥n")

# Opciones para informe
st.sidebar.markdown("### Opciones para informe")
generate_report_checkbox = st.sidebar.checkbox("Generar informe de la rueda", value=False)

# Market toggle (US ‚Üî AR) ‚Äî right below the report checkbox
is_argentina = st.sidebar.toggle("üá¶üá∑ Mercado Argentina", value=False, help="Desactivado = üá∫üá∏ EEUU ¬∑ Activado = üá¶üá∑ Argentina")
selected_market = "AR" if is_argentina else "US"
market_config = get_market_config(selected_market)

# Derive paths from market config
SYSTEM_PROMPT_PATH = market_config.system_prompt_path
RAG_PROMPT_PATH = market_config.rag_prompt_path
THRESHOLD_PATH = market_config.threshold_dataset_path
TICKER_MAP = market_config.ticker_map

date_picker = st.sidebar.date_input("Fecha del informe (default:hoy)", value=None)

# Debug toggle
enable_debug = st.sidebar.checkbox("üêõ Mostrar debug (conversaci√≥n agentes)", value=False)
st.sidebar.markdown("---")

# news inputs
news_text = st.sidebar.text_area("Texto de noticias (opcional)", value="", height=150)
news_urls = st.sidebar.text_area("URLs de noticias (una por l√≠nea)", value="", height=120)

k_total = st.sidebar.slider("Top-k total", 1, 30, 10, 1)
temperature = st.sidebar.slider("Temperature (OpenAI)", 0.0, 1.0, 0.0, 0.1)
openai_model = st.sidebar.text_input("Modelo OpenAI", value=DEFAULT_OAI_MODEL)
embed_model = st.sidebar.text_input("Modelo de Embeddings (HF)", value=DEFAULT_EMBED)
history_path = st.sidebar.text_input("Archivo de historial (JSONL)", value="./data/history/chat_history.jsonl")
show_sources = st.sidebar.checkbox("Mostrar fuentes", value=True)
show_scores = st.sidebar.checkbox("Mostrar score", value=False)
show_preview = st.sidebar.checkbox("Mostrar preview", value=True)

st.sidebar.markdown("---")
st.sidebar.caption("Requiere OPENAI_API_KEY si quer√©s usar verificador OpenAI.")

# --------- caches ----------
@st.cache_resource(show_spinner=False)
def get_embedder_cached(model_name: str):
    return HuggingFaceEmbeddings(model_name=model_name, encode_kwargs={"normalize_embeddings": True})

embedder = get_embedder_cached(embed_model)

# no vectordb usage: retrieval will use only `news_text`, `news_urls` and computed CSV

# session_state for variations
if "computed_variations" not in st.session_state:
    st.session_state["computed_variations"] = None
# date for which computed_variations was generated (YYYY-MM-DD or None)
if "computed_variations_date" not in st.session_state:
    st.session_state["computed_variations_date"] = None
# data date mode (la moda de las fechas de los datos)
if "data_date_mode" not in st.session_state:
    st.session_state["data_date_mode"] = None
# failed tickers
if "failed_tickers" not in st.session_state:
    st.session_state["failed_tickers"] = []
# market status warning
if "market_warning" not in st.session_state:
    st.session_state["market_warning"] = None

# Nota: el checkbox "Generar informe" solo indica usar el `systemprompt_template`.
# No se descarga/ejecuta `compute_variations` hasta que el usuario env√≠e la consulta.
if generate_report_checkbox:
    st.sidebar.info("Al enviar la consulta se usar√° el sistema de 'informe' (system prompt). El CSV se descargar√° solo si es necesario.")

# ----- Chat render previo (history) -----
if "messages" not in st.session_state:
    st.session_state.messages = []

for m in st.session_state.messages:
    with st.chat_message(m["role"]):
        st.markdown(m["content"])
        if show_sources and m.get("sources"):
            with st.expander("Fuentes"):
                for s in m.get("sources", []):
                    st.text(clean_text(s))

# ----- Main interaction -----
user_input = st.chat_input("Escrib√≠ tu pregunta (ej: \"Gener√° resumen para 16/01/2026\")")
if user_input:
    st.session_state.messages.append({"role":"user","content":user_input})
    with st.chat_message("user"):
        st.markdown(user_input)

    # build retrieval context (very similar to previous pipeline)
    candidates_all = []
    local_docs = []

    # add news_text as individual news items (do NOT chunk)
    if news_text and news_text.strip():
        # split on blank lines to separate distinct news items; fallback to whole text
        parts = [p.strip() for p in news_text.split("\n\n") if p.strip()]
        if not parts:
            parts = [news_text.strip()]
        for i, part in enumerate(parts, 1):
            local_docs.append(SimpleDoc(page_content=part, metadata={"_collection":"news_user","source":"news_text","news_id":i}))

    # add each URL as a single SimpleDoc (do NOT chunk)
    if news_urls and news_urls.strip():
        for i, url in enumerate([u.strip() for u in news_urls.splitlines() if u.strip()], 1):
            txt = fetch_url_text(url)
            if not txt:
                continue
            local_docs.append(SimpleDoc(page_content=txt, metadata={"_collection":"news_url","source":url,"news_id":i}))

    # add CSV rows (computed)
    # If the user selected 'Generar informe' (use system prompt) we compute the CSV
    df_computed = st.session_state.get("computed_variations")
    # determine date to request for compute: prefer date in user_input, otherwise date_picker
    extracted_date = None
    try:
        extracted_date = extract_date_from_text(user_input)
    except Exception:
        extracted_date = None
    compute_target_date = None
    if extracted_date:
        compute_target_date = extracted_date
    elif date_picker is not None:
        try:
            compute_target_date = date_picker.strftime("%Y-%m-%d")
        except Exception:
            compute_target_date = None
    else:
        # default to today's date if none provided
        try:
            compute_target_date = datetime.date.today().strftime("%Y-%m-%d")
        except Exception:
            compute_target_date = None

    # If user asked for system prompt / informe, compute CSV
    # or if the already-computed CSV was generated for a different date.
    existing_computed_date = st.session_state.get("computed_variations_date")
    existing_data_date_mode = st.session_state.get("data_date_mode")
    
    if generate_report_checkbox:
        need_compute = False
        if df_computed is None:
            need_compute = True
        else:
            # Comparar fechas: recalcular si la fecha solicitada cambi√≥
            if existing_computed_date != compute_target_date:
                need_compute = True
            # TAMBI√âN recalcular si los datos existentes son de una fecha anterior a la solicitada
            # (por si el mercado ya abri√≥ desde la √∫ltima vez que se calcul√≥)
            elif existing_data_date_mode and compute_target_date:
                data_mode_date = datetime.datetime.strptime(existing_data_date_mode, "%Y-%m-%d").date()
                requested_date = datetime.datetime.strptime(compute_target_date, "%Y-%m-%d").date()
                if data_mode_date < requested_date:
                    # Los datos son viejos, intentar recalcular por si el mercado ya abri√≥
                    need_compute = True
                    
        if need_compute:
            try:
                df_out, close, data_date_mode, failed_tickers = compute_variations(TICKER_MAP, lookback="30d", target_date=compute_target_date)
                st.session_state["computed_variations"] = df_out
                # store the date used for this computation (can be None)
                st.session_state["computed_variations_date"] = compute_target_date
                st.session_state["data_date_mode"] = data_date_mode
                st.session_state["failed_tickers"] = failed_tickers
                df_computed = df_out
                
                # Verificar si hay tickers que fallaron
                if failed_tickers:
                    st.warning(f"‚ö†Ô∏è Los siguientes tickers fallaron en la descarga despu√©s de reintentos: {', '.join(failed_tickers)}")
                
                # Verificar si el mercado abri√≥ comparando la fecha solicitada con la moda de fechas
                if data_date_mode and compute_target_date:
                    requested_date = datetime.datetime.strptime(compute_target_date, "%Y-%m-%d").date()
                    data_mode_date = datetime.datetime.strptime(data_date_mode, "%Y-%m-%d").date()
                    
                    if requested_date > data_mode_date:
                        # El mercado todav√≠a no abri√≥ para la fecha solicitada
                        warning_msg = f"‚ö†Ô∏è **ATENCI√ìN**: El mercado de EEUU todav√≠a no abri√≥ para el {requested_date.strftime('%d/%m/%Y')}. Los datos disponibles son del {data_mode_date.strftime('%d/%m/%Y')}. El informe se generar√° con los datos del d√≠a anterior."
                        st.session_state["market_warning"] = warning_msg
                        st.warning(warning_msg)
                    else:
                        st.session_state["market_warning"] = None
                
                st.info("CSV calculado y almacenado en sesi√≥n (computed_variations).")
            except Exception as e:
                st.warning(f"No se pudo descargar el CSV: {e}")

    df_use = df_computed
    if df_use is not None:
        # crea un √∫nico SimpleDoc que contiene TODO el CSV
        single_doc = df_to_single_doc(
            df_use,
            source_name="variacion_diaria.csv",
            extra_meta={"_collection": "tickers_csv", "source": "csv"}
        )
        local_docs.append(single_doc)

    # try to embed local_docs and add to candidates_all (so they are considered for retrieval)
    try:
        embedder_local = embedder
        texts = [d.page_content for d in local_docs]
        if texts:
            # compute query embedding (prefer embed_query, fallback to embed_documents)
            query_emb = None
            try:
                if hasattr(embedder_local, "embed_query"):
                    query_emb = embedder_local.embed_query(user_input)
                else:
                    q_embs = embedder_local.embed_documents([user_input])
                    query_emb = q_embs[0] if q_embs else None
            except Exception:
                query_emb = None

            # compute embeddings for local docs
            try:
                emb_list = embedder_local.embed_documents(texts)
            except Exception:
                emb_list = []

            import numpy as _np
            def cos(a,b):
                a=_np.array(a); b=_np.array(b)
                if a.size==0 or b.size==0: return 0.0
                na=_np.linalg.norm(a); nb=_np.linalg.norm(b)
                if na==0 or nb==0: return 0.0
                return float(_np.dot(a,b)/(na*nb))

            if emb_list and query_emb is not None:
                for d, emb in zip(local_docs, emb_list):
                    sim = cos(query_emb, emb)
                    dist = 1.0 - sim
                    candidates_all.append((d, dist))
            else:
                # fallback: include local_docs with neutral distance so they appear in context
                for d in local_docs:
                    candidates_all.append((d, 1.0))
    except Exception:
        # ignore embed errors but ensure local_docs are still considered
        for d in local_docs:
            candidates_all.append((d, 1.0))

    # Build top docs (simple scoring: use dist as score proxy, no recency for brevity)
    scored = []
    for doc, dist in candidates_all:
        scored.append((doc, dist, 1.0/(1.0+dist)))
    scored.sort(key=lambda x: x[2], reverse=True)
    top = scored[:k_total]
    top_docs = [doc for (doc,_,_) in top]

    # Build context string for RAG
    context_parts = []
    for i, doc in enumerate(top_docs,1):
        context_parts.append(f"[{i}] ({doc.metadata.get('_collection')})\n{doc.page_content}")
    context = "\n\n".join(context_parts)


    # If generate_report_checkbox is True -> use systemprompt_template and include the CSV (if exists)
    use_system_prompt = generate_report_checkbox and os.path.exists(SYSTEM_PROMPT_PATH)
    # Ensure the date for the summary is present in the question passed to the system prompt.
    # Prefer an explicit date in the user_input (extracted), otherwise use the date_picker if provided.
    try:
        extracted_date = extract_date_from_text(user_input)
    except Exception:
        extracted_date = None
    
    # Diccionario para d√≠as de la semana en espa√±ol
    DIAS_SEMANA = ["lunes", "martes", "mi√©rcoles", "jueves", "viernes", "s√°bado", "domingo"]
    
    # Verificar si el mercado no abri√≥ y debemos usar la fecha de los datos reales
    market_warning = st.session_state.get("market_warning")
    data_date_mode = st.session_state.get("data_date_mode")
    
    # Determinar la fecha real a usar para el informe
    # Si el mercado no abri√≥, usar la fecha de los datos (data_date_mode)
    if market_warning and data_date_mode:
        # Usar la fecha de los datos reales (d√≠a anterior)
        report_date_obj = datetime.datetime.strptime(data_date_mode, "%Y-%m-%d").date()
        dia_semana = DIAS_SEMANA[report_date_obj.weekday()]
        qdate = f"{report_date_obj.strftime('%d/%m/%Y')} ({dia_semana})"
        market_note = f"\n\n[NOTA: El mercado de EEUU todav√≠a no abri√≥ para la fecha solicitada. Los datos corresponden al {qdate}. Us√° esta fecha para el informe.]"
    else:
        # Usar la fecha extra√≠da, date_picker, o la fecha de los datos si est√° disponible
        if extracted_date:
            report_date_obj = datetime.datetime.strptime(extracted_date, "%Y-%m-%d").date()
        elif date_picker is not None:
            report_date_obj = date_picker
        elif data_date_mode:
            # Usar la fecha real de los datos del CSV
            report_date_obj = datetime.datetime.strptime(data_date_mode, "%Y-%m-%d").date()
        else:
            report_date_obj = datetime.date.today()
        dia_semana = DIAS_SEMANA[report_date_obj.weekday()]
        qdate = f"{report_date_obj.strftime('%d/%m/%Y')} ({dia_semana})"
        market_note = ""
    
    # Construir question_for_prompt con la fecha y d√≠a de la semana correctos
    if user_input and user_input.strip():
        question_for_prompt = f"{user_input} para {qdate}"
    else:
        question_for_prompt = f"Gener√° resumen para {qdate}"
    
    # Agregar nota de mercado si corresponde
    if market_note:
        question_for_prompt = question_for_prompt + market_note
    
    if use_system_prompt:
        # build CSV textual block
        if df_use is None:
            st.warning("No hay CSV disponible: sub√≠ un CSV o presion√° 'Calcular CSV' en la barra lateral.")
        csv_block = format_variations_for_prompt(df_use) if df_use is not None else ""
        
        # Filtrar el CSV del context para no duplicarlo (ya est√° en csv_block)
        context_without_csv = "\n\n".join([
            part for part in context_parts 
            if "tickers_csv" not in part and "variacion_diaria.csv" not in part
        ])
        
        # final context: CSV + news/docs (sin duplicar el CSV)
        if context_without_csv.strip():
            merged_context = csv_block + "\n\n" + context_without_csv
        else:
            merged_context = csv_block
            
        try:
            with open(SYSTEM_PROMPT_PATH, "r", encoding="utf-8") as fh:
                system_prompt_template = fh.read()
        except Exception:
            st.error("No se pudo cargar system prompt.")
            system_prompt_template = "{context}\n\n{question}"
        system_prompt = system_prompt_template.format(context=merged_context, question=question_for_prompt)
    else:
        # normal RAG prompt
        try:
            with open(RAG_PROMPT_PATH, "r", encoding="utf-8") as fh:
                rag_prompt_template = fh.read()
        except Exception:
            rag_prompt_template = "{context}\n\n{question}"
        system_prompt = rag_prompt_template.format(context=context, question=question_for_prompt)

    # Preparar los mensajes que se enviar√°n al modelo
    llm_messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": question_for_prompt}
    ]

    # Preparar evaluaci√≥n si es informe
    ds = None
    few_shot = None
    reference_response = None
    query_date = None
    if use_system_prompt and df_use is not None:
        ds = load_dataset(THRESHOLD_PATH)
        few_shot = ds[:3]
        prompt_text = format_variations_for_prompt(df_use)
        query_date = extract_date_from_prompt(prompt_text)
        if query_date:
            ref_entry = find_reference_for_date(ds, query_date)
            if ref_entry:
                reference_response = ref_entry.get("response")

    # call LLM (OpenAI) to generate answer - with retry if score < threshold
    MAX_RETRIES = market_config.max_eval_retries
    MIN_SCORE = market_config.min_eval_score
    best_answer = None
    best_score = 0.0
    best_eval_res = None
    attempts = []

    # Initialise debug session
    debug_session = DebugSession()
    debug_session.start(market_id=market_config.market_id, target_date=compute_target_date or "")
    
    with st.spinner("Generando respuesta con LLM..."):
        if OpenAI is None:
            answer = "‚ö†Ô∏è OpenAI SDK no disponible en el entorno. Configura OPENAI_API_KEY o instala openai."
        else:
            client = OpenAI()
            # Datos CSV formateados para re-anclar en cada reintento
            csv_data_for_eval = ""
            if use_system_prompt and df_use is not None:
                csv_data_for_eval = format_variations_for_prompt(df_use)
                if news_text:
                    csv_data_for_eval += f"\n\nNoticias:\n{news_text}"

            accumulated_feedback: list[dict] = []  # Historial de correcciones estructuradas
            eval_history: list[dict] = []  # Historial completo para el evaluador

            for attempt in range(MAX_RETRIES):
                try:
                    # Temperatura: subir muy poco (+0.03/intento, max +0.12)
                    retry_temp = min(temperature + (attempt * 0.03), temperature + 0.12)

                    if attempt == 0:
                        # Primera iteraci√≥n: prompt original
                        send_messages = llm_messages.copy()
                    else:
                        # Construir bloque de feedback hol√≠stico del evaluador
                        feedback_parts = []
                        for i, fb in enumerate(accumulated_feedback):
                            part = f"--- Intento {i+1} (score {fb['score']:.2f}) ---"
                            if fb.get('reason'):
                                part += f"\nAn√°lisis del evaluador: {fb['reason']}"
                            if fb.get('datos_correctos') is False:
                                part += "\n‚ö†Ô∏è HAY VALORES NUM√âRICOS INCORRECTOS. Verific√° cada dato contra el CSV."
                            if fb.get('narrativa_quality'):
                                part += f"\nCalidad narrativa: {fb['narrativa_quality']}"
                            if fb.get('mejoras'):
                                part += "\nMEJORAS PRIORITARIAS:"
                                for mejora in fb['mejoras']:
                                    part += f"\n  ‚Ä¢ {mejora}"
                            feedback_parts.append(part)

                        feedback_block = "\n\n".join(feedback_parts)

                        retry_user_msg = f"""{question_for_prompt}

ATENCI√ìN: Este es el intento {attempt + 1}. El evaluador encontr√≥ aspectos a mejorar.

=== DATOS CSV DE REFERENCIA (verific√° tus valores contra estos) ===
{csv_data_for_eval}

=== FEEDBACK DEL EVALUADOR (intentos anteriores) ===
{feedback_block}

=== INSTRUCCIONES ===
1. Le√© el feedback del evaluador y aplic√° las mejoras prioritarias.
2. Verific√° que los valores num√©ricos que mencion√©s coincidan con el CSV.
3. Prioriz√° la narrativa: explic√° POR QU√â se movi√≥ el mercado, no solo list√©s tickers.
4. Integr√° noticias y datos macro como causas de los movimientos.
5. Manten√© estructura profesional (p√°rrafos fluidos, no bullet points).

Gener√° la respuesta mejorada ahora:"""
                        send_messages = [
                            {"role": "system", "content": system_prompt},
                            {"role": "user", "content": retry_user_msg},
                        ]

                    resp = client.chat.completions.create(
                        model=openai_model,
                        messages=send_messages,
                        temperature=min(retry_temp, 1.0)
                    )
                    answer = resp.choices[0].message.content
                    
                    # Evaluar solo si es informe
                    if use_system_prompt and df_use is not None and few_shot:
                        eval_prompt = build_eval_prompt(
                            few_shot, csv_data_for_eval, answer, reference_response,
                            iteration=attempt + 1,
                            previous_attempts=eval_history if eval_history else None,
                        )
                        eval_res, eval_raw = call_evaluator(eval_prompt, openai_model=openai_model, temperature=0.0)
                        score = eval_res.get("score", 0.0)
                        reason = eval_res.get("reason", "")
                        attempts.append({"attempt": attempt + 1, "score": score, "reason": reason})

                        # Record in debug session
                        writer_user_msg = send_messages[-1]["content"]
                        debug_session.add_iteration(
                            iteration=attempt + 1,
                            writer_system=system_prompt,
                            writer_user=writer_user_msg,
                            writer_response=answer,
                            writer_temperature=retry_temp,
                            evaluator_prompt=eval_prompt,
                            evaluator_raw=eval_raw,
                            eval_score=score,
                            eval_ok=score >= MIN_SCORE,
                            eval_reason=reason,
                        )
                        
                        # Guardar mejor respuesta
                        if score > best_score:
                            best_score = score
                            best_answer = answer
                            best_eval_res = eval_res
                        
                        # Si alcanzamos el umbral, salir
                        if score >= MIN_SCORE:
                            break
                        
                        # Acumular feedback hol√≠stico para el pr√≥ximo intento
                        if attempt < MAX_RETRIES - 1:
                            accumulated_feedback.append({
                                "score": score,
                                "reason": reason,
                                "datos_correctos": eval_res.get("datos_correctos", True),
                                "narrativa_quality": eval_res.get("narrativa_quality", ""),
                                "mejoras": eval_res.get("mejoras", []),
                            })
                            eval_history.append({
                                "iteration": attempt + 1,
                                "response": answer,
                                "score": score,
                                "reason": reason,
                                "mejoras": eval_res.get("mejoras", []),
                            })
                    else:
                        # No es informe, no evaluar
                        best_answer = answer
                        break
                        
                except Exception as e:
                    answer = f"‚ö†Ô∏è Error llamando a OpenAI: {e}"
                    best_answer = answer
                    break
            
            # Usar la mejor respuesta encontrada
            if best_answer:
                answer = best_answer

    # Finish debug session
    debug_session.finish(final_answer=answer, final_score=best_score)

    # show sources
    sources = []
    for i, (doc, dist, cscore) in enumerate(top, 1):
        tag = f"{doc.metadata.get('_collection')} - {doc.metadata.get('source','')}"
        preview = (doc.page_content[:240] + "‚Ä¶") if len(doc.page_content)>240 else doc.page_content
        sources.append(f"[{i}] ({tag})\n> {preview}")

    # render answer
    with st.chat_message("assistant"):
        st.markdown(answer)
        if show_sources and sources:
            with st.expander("Fuentes"):
                for s in sources:
                    st.write(s)

    st.session_state.messages.append({"role":"assistant","content":answer,"sources":sources})

    # If we generated an "informe", show evaluation results (already computed during generation)
    if use_system_prompt and df_use is not None:
        # Guardar en session_state para que persista al hacer click en botones
        st.session_state["last_eval_result"] = {
            "score": best_score,
            "eval_res": best_eval_res,
            "answer": answer,
            "df_use": df_use,
            "news_text": news_text,
            "attempts": attempts,
            "query_date": query_date,
            "reference_response": reference_response
        }
        # Store debug session in session_state so it survives reruns
        st.session_state["last_debug_session"] = debug_session
        
        if query_date and reference_response:
            st.sidebar.info(f"üìä Usando referencia del dataset para {query_date}")
        
        # Mostrar intentos de generaci√≥n
        if attempts:
            st.sidebar.markdown("### üîÑ Intentos de generaci√≥n")
            for att in attempts:
                emoji = "‚úÖ" if att["score"] >= MIN_SCORE else "‚ö†Ô∏è"
                st.sidebar.write(f"{emoji} Intento {att['attempt']}: Score **{att['score']:.2f}**")
            st.sidebar.write(f"Total de intentos: **{len(attempts)}**")
        
        # Usar los resultados de evaluaci√≥n ya calculados
        if best_eval_res:
            score = best_eval_res.get("score", 0.0)
            ok = score >= MIN_SCORE
            reason = best_eval_res.get("reason", "")
        else:
            score = 0.0
            ok = False
            reason = "No se pudo evaluar"
        
        st.sidebar.markdown("### Evaluaci√≥n del informe")
        if score >= MIN_SCORE:
            st.sidebar.success(f"‚úÖ Score: **{score:.2f}** ‚Äî Aprobado")
        else:
            st.sidebar.warning(f"‚ö†Ô∏è Score: **{score:.2f}** ‚Äî Mejor resultado despu√©s de {len(attempts)} intentos")
        st.sidebar.write(f"Motivo: {reason}")

# ‚îÄ‚îÄ‚îÄ Debug visual: timeline de agentes ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
if enable_debug and "last_debug_session" in st.session_state and st.session_state["last_debug_session"]:
    dsess: DebugSession = st.session_state["last_debug_session"]

    st.markdown("---")
    # Header con resumen visual
    score_color = "üü¢" if dsess.final_score >= market_config.min_eval_score else "üî¥"
    st.markdown(
        f"## üêõ Debug ‚Äî Conversaci√≥n entre agentes\n"
        f"**Iteraciones:** {dsess.total_iterations}  ¬∑  "
        f"**Score final:** {score_color} {dsess.final_score:.2f}"
    )

    for it in dsess.iterations:
        iter_emoji = "‚úÖ" if it.eval_ok else "üîÑ"
        score_bar_pct = int(it.eval_score * 100)

        st.markdown(f"---\n### {iter_emoji} Iteraci√≥n {it.iteration} de {dsess.total_iterations}")
        # Score progress bar
        st.progress(min(it.eval_score, 1.0), text=f"Score: {it.eval_score:.2f}  ¬∑  Temp: {it.writer_temperature:.2f}")

        # ‚îÄ‚îÄ MODELO ESCRITOR ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        st.markdown("#### ‚úçÔ∏è Modelo Escritor")

        if it.iteration == 1:
            st.caption("üìå Primera iteraci√≥n ‚Äî sin correcciones previas")
        else:
            st.caption(f"üìå Iteraci√≥n {it.iteration} ‚Äî con correcciones del evaluador")

        # Lo que le llega al escritor (prompt)
        with st.expander("üì• Prompt que recibe el Modelo Escritor", expanded=False):
            st.markdown("**System prompt:**")
            st.text_area(
                "system_writer", it.writer_prompt_system,
                height=200, disabled=True, label_visibility="collapsed",
                key=f"dbg_writer_sys_{it.iteration}"
            )
            label_user = "User prompt (pregunta original)" if it.iteration == 1 else "User prompt (con feedback / correcciones)"
            st.markdown(f"**{label_user}:**")
            st.text_area(
                "user_writer", it.writer_prompt_user,
                height=150, disabled=True, label_visibility="collapsed",
                key=f"dbg_writer_usr_{it.iteration}"
            )

        # Lo que responde el escritor
        st.markdown("**üì§ Respuesta del Modelo Escritor:**")
        st.info(it.writer_response)

        # ‚îÄ‚îÄ MODELO EVALUADOR ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
        st.markdown("#### üîç Modelo Evaluador")

        # Lo que le llega al evaluador (prompt)
        with st.expander("üì• Prompt que recibe el Modelo Evaluador", expanded=False):
            st.text_area(
                "eval_prompt", it.evaluator_prompt,
                height=250, disabled=True, label_visibility="collapsed",
                key=f"dbg_eval_prompt_{it.iteration}"
            )

        # Lo que responde el evaluador
        st.markdown("**üì§ Respuesta del Modelo Evaluador:**")
        # Parsear la respuesta del evaluador para mostrarla de forma legible
        try:
            eval_parsed = json.loads(it.evaluator_raw_response)
            eval_cols = st.columns([1, 1, 1])
            with eval_cols[0]:
                s = eval_parsed.get("score", 0)
                st.metric("Score", f"{s:.2f}", delta=None)
            with eval_cols[1]:
                datos_ok = eval_parsed.get("datos_correctos", None)
                st.metric("Datos correctos", "‚úÖ S√≠" if datos_ok else "‚ùå No")
            with eval_cols[2]:
                narrativa = eval_parsed.get("narrativa_quality", "?")
                emoji_narr = {"alta": "üü¢", "media": "üü°", "baja": "üî¥"}.get(narrativa, "‚ö™")
                st.metric("Narrativa", f"{emoji_narr} {narrativa}")

            # An√°lisis hol√≠stico
            reason = eval_parsed.get("reason", "")
            if reason:
                st.info(f"üí¨ **An√°lisis:** {reason}")

            # Mejoras sugeridas
            mejoras = eval_parsed.get("mejoras", [])
            if mejoras:
                st.markdown("**üìã Mejoras prioritarias:**")
                for j, mejora in enumerate(mejoras, 1):
                    st.markdown(f"{j}. {mejora}")
            else:
                st.success("Sin mejoras sugeridas ‚Äî informe excelente")

        except (json.JSONDecodeError, TypeError):
            # Fallback: mostrar raw si no se puede parsear
            st.code(it.evaluator_raw_response, language="json")
            if it.eval_reason:
                st.caption(f"üí¨ {it.eval_reason}")

    # Footer
    st.markdown("---")
    final_emoji = "‚úÖ" if dsess.final_score >= market_config.min_eval_score else "‚ö†Ô∏è"
    st.markdown(
        f"### {final_emoji} Resultado final: score **{dsess.final_score:.2f}** "
        f"en **{dsess.total_iterations}** iteraci√≥n(es)"
    )

# Bot√≥n para agregar al dataset (fuera del bloque de generaci√≥n, usa session_state)
if "last_eval_result" in st.session_state and st.session_state["last_eval_result"]:
    eval_data = st.session_state["last_eval_result"]
    if eval_data.get("score", 0) >= market_config.min_eval_score:
        if st.sidebar.button("‚ûï Agregar ejemplo al dataset"):
            try:
                df_use_saved = eval_data["df_use"]
                news_text_saved = eval_data.get("news_text", "")
                answer_saved = eval_data["answer"]
                score_saved = eval_data["score"]
                rec = {
                    "prompt": format_variations_for_prompt(df_use_saved) + "\n\nNoticias:\n" + (news_text_saved or ""),
                    "response": answer_saved,
                    "accuracy": score_saved
                }
                with open(THRESHOLD_PATH, "a", encoding="utf-8") as fh:
                    fh.write(json.dumps(rec, ensure_ascii=False) + "\n")
                st.sidebar.success("‚úÖ Ejemplo agregado al dataset.")
                # Limpiar para no agregar duplicados
                st.session_state["last_eval_result"] = None
            except Exception as e:
                st.sidebar.error(f"Error guardando: {e}")

    # save history
    try:
        os.makedirs(os.path.dirname(history_path) or ".", exist_ok=True)
        with open(history_path, "a", encoding="utf-8") as fh:
            fh.write(json.dumps({
                "timestamp": datetime.datetime.now().isoformat(),
                "query": user_input,
                "answer": answer,
                "sources": sources,
                "generate_report": bool(generate_report_checkbox)
            }, ensure_ascii=False) + "\n")
    except Exception:
        pass
